{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XqxffiyTq60O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Optional\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0ztKQberWvG"
      },
      "outputs": [],
      "source": [
        "class TicTacToeEnv:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes a new TicTacToe environment.\n",
        "\n",
        "        Board representation:\n",
        "        - 0: Empty cell\n",
        "        - 1: Player X\n",
        "        - -1: Player O (opponent)\n",
        "\n",
        "        The board is initialized as a 3x3 matrix of zeros.\n",
        "        Player X starts the game (represented by 1).\n",
        "        \"\"\"\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Resets the environment for a new game.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: A clean 3x3 board filled with zeros\n",
        "        \"\"\"\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "        return self.board.copy()\n",
        "\n",
        "    def get_valid_moves(self) -> List[Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Returns all valid moves in the current board state.\n",
        "\n",
        "        Returns:\n",
        "            List[Tuple[int, int]]: List of (row, column) tuples representing empty cells\n",
        "        \"\"\"\n",
        "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
        "\n",
        "    def make_move(self, position: Tuple[int, int]) -> Tuple[np.ndarray, float, bool]:\n",
        "        \"\"\"\n",
        "        Executes a move on the board and returns the new state.\n",
        "\n",
        "        Args:\n",
        "            position: Tuple[int, int] representing (row, column) of the move\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - np.ndarray: New board state\n",
        "            - float: Reward (-1 for loss, 0 for ongoing, 1 for win)\n",
        "            - bool: Whether the game is finished\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the move is invalid (cell already occupied)\n",
        "        \"\"\"\n",
        "        if not self._is_valid_move(position):\n",
        "            raise ValueError(\"Invalid move: Cell is already occupied\")\n",
        "\n",
        "        self.board[position] = self.current_player\n",
        "\n",
        "        done = False\n",
        "        reward = 0\n",
        "\n",
        "        if self._check_winner() == self.current_player:\n",
        "            reward = 1\n",
        "            done = True\n",
        "        elif self._check_winner() is not None:\n",
        "            reward = -1\n",
        "            done = True\n",
        "        elif len(self.get_valid_moves()) == 0:\n",
        "            done = True\n",
        "\n",
        "        self.current_player *= -1\n",
        "        return self.board.copy(), reward, done\n",
        "\n",
        "    def _is_valid_move(self, position: Tuple[int, int]) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if a move is valid.\n",
        "\n",
        "        Args:\n",
        "            position: Tuple[int, int] representing (row, column) of the move\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the cell is empty (0), False otherwise\n",
        "        \"\"\"\n",
        "        return self.board[position] == 0\n",
        "\n",
        "    def _check_winner(self) -> Optional[int]:\n",
        "        \"\"\"\n",
        "        Checks if there's a winner in the current board state.\n",
        "\n",
        "        Winning conditions:\n",
        "        - Three in a row (horizontally)\n",
        "        - Three in a column (vertically)\n",
        "        - Three in a diagonal\n",
        "\n",
        "        Returns:\n",
        "            Optional[int]:\n",
        "            - 1 if X wins\n",
        "            - -1 if O wins\n",
        "            - None if no winner yet\n",
        "        \"\"\"\n",
        "        for i in range(3):\n",
        "            if abs(sum(self.board[i,:])) == 3:\n",
        "                return self.board[i,0]\n",
        "\n",
        "        for j in range(3):\n",
        "            if abs(sum(self.board[:,j])) == 3:\n",
        "                return self.board[0,j]\n",
        "\n",
        "        if abs(sum(np.diag(self.board))) == 3:\n",
        "            return self.board[0,0]\n",
        "        if abs(sum(np.diag(np.fliplr(self.board)))) == 3:\n",
        "            return self.board[0,2]\n",
        "\n",
        "        return None\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Displays the current board state in a human-readable format.\n",
        "\n",
        "        Output format:\n",
        "        ---------\n",
        "        | X | O | X |\n",
        "        ---------\n",
        "        | O | X | O |\n",
        "        ---------\n",
        "        | X | O | X |\n",
        "        ---------\n",
        "        \"\"\"\n",
        "        symbols = {0: ' ', 1: 'X', -1: 'O'}\n",
        "        print('---------')\n",
        "        for i in range(3):\n",
        "            print('|', end=' ')\n",
        "            for j in range(3):\n",
        "                print(symbols[self.board[i,j]], end=' | ')\n",
        "            print('\\n---------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X5XjDWKVrbcK"
      },
      "outputs": [],
      "source": [
        "def test_environment():\n",
        "    \"\"\"\n",
        "    Demonstrates the functionality of the TicTacToe environment through a sample game.\n",
        "\n",
        "    This function:\n",
        "    1. Creates a new environment instance\n",
        "    2. Executes a predefined sequence of moves\n",
        "    3. Displays the board state after each move\n",
        "    4. Shows rewards and game status\n",
        "\n",
        "    Test sequence:\n",
        "    - Move (0,0): Top-left corner\n",
        "    - Move (1,1): Center\n",
        "    - Move (0,1): Top-middle\n",
        "    - Move (0,2): Top-right\n",
        "    - Move (2,2): Bottom-right\n",
        "\n",
        "    Output for each move:\n",
        "    - Visual representation of the board\n",
        "    - Position of the move\n",
        "    - Reward received (-1, 0, or 1)\n",
        "    - Whether the game has ended\n",
        "\n",
        "    The test terminates early if the game ends (win/draw/loss).\n",
        "\n",
        "    Example output:\n",
        "    ---------\n",
        "    | X |   |   |\n",
        "    ---------\n",
        "    |   |   |   |\n",
        "    ---------\n",
        "    |   |   |   |\n",
        "    ---------\n",
        "    Reward: 0, Game ended: False\n",
        "    \"\"\"\n",
        "    # Initialize new environment\n",
        "    env = TicTacToeEnv()\n",
        "    print(\"Initial state:\")\n",
        "    env.render()\n",
        "\n",
        "    # List of moves to test (row, column)\n",
        "    moves = [(0,0), (1,1), (0,1), (0,2), (2,2)]\n",
        "\n",
        "    # Execute each move and display results\n",
        "    for move in moves:\n",
        "        print(f\"\\nMove at position {move}:\")\n",
        "        board, reward, done = env.make_move(move)\n",
        "        env.render()\n",
        "        print(f\"Reward: {reward}, Game ended: {done}\")\n",
        "\n",
        "        # Stop if game is finished\n",
        "        if done:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "0C9yiMqkuAK6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Tuple, Dict, List\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "class TicTacToeQLearningAgent:\n",
        "    \"\"\"Q-learning agent for playing TicTacToe.\"\"\"\n",
        "\n",
        "    def __init__(self, epsilon: float = 0.1, alpha: float = 0.2, gamma: float = 0.3):\n",
        "        \"\"\"\n",
        "        Initialize Q-learning agent.\n",
        "\n",
        "        Args:\n",
        "            epsilon: Exploration rate\n",
        "            alpha: Learning rate\n",
        "            gamma: Discount factor\n",
        "        \"\"\"\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.training_stats = {'wins': 0, 'losses': 0, 'draws': 0}\n",
        "\n",
        "    def _get_state_key(self, board: np.ndarray) -> str:\n",
        "        \"\"\"Convert board state to string key for Q-table.\"\"\"\n",
        "        return ','.join(map(str, board.flatten()))\n",
        "\n",
        "    def get_action(self, board: np.ndarray, valid_moves: List[Tuple[int, int]]) -> Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Choose action using epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "            board: Current game state\n",
        "            valid_moves: List of available moves\n",
        "\n",
        "        Returns:\n",
        "            Chosen move as (row, col)\n",
        "        \"\"\"\n",
        "        state = self._get_state_key(board)\n",
        "\n",
        "        # Initialize Q-values for new state-action pairs\n",
        "        for move in valid_moves:\n",
        "            if move not in self.q_table[state]:\n",
        "                self.q_table[state][move] = np.random.uniform(0, 0.1)\n",
        "\n",
        "        # Exploration\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(valid_moves)\n",
        "\n",
        "        # Exploitation with randomized tie-breaking\n",
        "        q_values = {move: self.q_table[state][move] for move in valid_moves}\n",
        "        max_q = max(q_values.values())\n",
        "        best_moves = [move for move, q in q_values.items() if q == max_q]\n",
        "        return random.choice(best_moves)\n",
        "\n",
        "    def update(self, state: np.ndarray, action: Tuple[int, int],\n",
        "               next_state: np.ndarray, reward: float, done: bool,\n",
        "               next_valid_moves: List[Tuple[int, int]]):\n",
        "        \"\"\"\n",
        "        Update Q-values using Q-learning update rule.\n",
        "\n",
        "        Args:\n",
        "            state: Current state\n",
        "            action: Chosen action\n",
        "            next_state: Resulting state\n",
        "            reward: Reward received\n",
        "            done: Whether game ended\n",
        "            next_valid_moves: Available moves in next state\n",
        "        \"\"\"\n",
        "        state_key = self._get_state_key(state)\n",
        "        next_state_key = self._get_state_key(next_state)\n",
        "\n",
        "        # Initialize Q-values for next state\n",
        "        for move in next_valid_moves:\n",
        "            if move not in self.q_table[next_state_key]:\n",
        "                self.q_table[next_state_key][move] = np.random.uniform(0, 0.1)\n",
        "\n",
        "        # Get max Q-value for next state\n",
        "        next_max_q = max([self.q_table[next_state_key][next_action]\n",
        "                         for next_action in next_valid_moves]) if not done else 0\n",
        "\n",
        "        # Update Q-value\n",
        "        current_q = self.q_table[state_key][action]\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * next_max_q - current_q)\n",
        "        self.q_table[state_key][action] = new_q\n",
        "\n",
        "        # Update training statistics\n",
        "        if done:\n",
        "            if reward == 1:\n",
        "                self.training_stats['wins'] += 1\n",
        "            elif reward == -1:\n",
        "                self.training_stats['losses'] += 1\n",
        "            else:\n",
        "                self.training_stats['draws'] += 1\n",
        "\n",
        "    def save_policy(self, filepath: str = None) -> str:\n",
        "      \"\"\"\n",
        "      Save the Q-table policy to a pickle file.\n",
        "\n",
        "      Args:\n",
        "          filepath: Optional specific path to save the policy\n",
        "                  If None, generates a timestamp-based filename\n",
        "\n",
        "      Returns:\n",
        "          str: Path where the policy was saved\n",
        "      \"\"\"\n",
        "      # Convert defaultdict to regular dict for saving\n",
        "      policy_dict = {\n",
        "          'q_table': dict(self.q_table),\n",
        "          'epsilon': self.epsilon,\n",
        "          'alpha': self.alpha,\n",
        "          'gamma': self.gamma\n",
        "      }\n",
        "\n",
        "      # Generate filename with timestamp if not provided\n",
        "      if filepath is None:\n",
        "          timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "          filepath = f\"tictactoe_policy_{timestamp}.pkl\"\n",
        "\n",
        "      # Ensure the directory exists\n",
        "      os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
        "\n",
        "      # Save the policy\n",
        "      with open(filepath, 'wb') as f:\n",
        "          pickle.dump(policy_dict, f)\n",
        "\n",
        "      print(f\"Policy saved to: {filepath}\")\n",
        "      return filepath\n",
        "\n",
        "    @classmethod\n",
        "    def load_policy(cls, filepath: str) -> 'TicTacToeQLearningAgent':\n",
        "        \"\"\"\n",
        "        Load a Q-table policy from a pickle file.\n",
        "\n",
        "        Args:\n",
        "            filepath: Path to the saved policy file\n",
        "\n",
        "        Returns:\n",
        "            TicTacToeQLearningAgent: New agent instance with loaded policy\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If policy file doesn't exist\n",
        "        \"\"\"\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(f\"Policy file not found: {filepath}\")\n",
        "\n",
        "        # Load the policy\n",
        "        with open(filepath, 'rb') as f:\n",
        "            policy_dict = pickle.load(f)\n",
        "\n",
        "        # Create new agent with loaded parameters\n",
        "        agent = cls(\n",
        "            epsilon=policy_dict['epsilon'],\n",
        "            alpha=policy_dict['alpha'],\n",
        "            gamma=policy_dict['gamma']\n",
        "        )\n",
        "\n",
        "        # Convert regular dict back to defaultdict\n",
        "        agent.q_table = defaultdict(lambda: defaultdict(float),\n",
        "                                  {k: defaultdict(float, v)\n",
        "                                  for k, v in policy_dict['q_table'].items()})\n",
        "\n",
        "        print(f\"Policy loaded from: {filepath}\")\n",
        "        return agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(episodes: int = 50000, save_path: str = None) -> TicTacToeQLearningAgent:\n",
        "    \"\"\"\n",
        "    Train the Q-learning agent and optionally save the policy.\n",
        "\n",
        "    Args:\n",
        "        episodes: Number of training episodes\n",
        "        save_path: Optional path to save the trained policy\n",
        "                  If None, generates a timestamp-based filename\n",
        "\n",
        "    Returns:\n",
        "        Trained agent\n",
        "    \"\"\"\n",
        "    env = TicTacToeEnv()\n",
        "    agent = TicTacToeQLearningAgent()\n",
        "\n",
        "    # Training monitoring\n",
        "    episode_rewards = []\n",
        "    win_rates = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            valid_moves = env.get_valid_moves()\n",
        "            action = agent.get_action(state, valid_moves)\n",
        "\n",
        "            next_state, reward, done = env.make_move(action)\n",
        "            next_valid_moves = env.get_valid_moves()\n",
        "\n",
        "            # Modify rewards to encourage winning\n",
        "            if reward == 1:\n",
        "                reward = 1\n",
        "            elif reward == -1:\n",
        "                reward = -2\n",
        "\n",
        "            agent.update(state, action, next_state, reward, done, next_valid_moves)\n",
        "            episode_reward += reward\n",
        "\n",
        "            state = next_state\n",
        "            if not done:\n",
        "                state = -state  # Flip board perspective\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "\n",
        "        # Print progress every 1000 episodes\n",
        "        if (episode + 1) % 1000 == 0:\n",
        "            win_rate = agent.training_stats['wins'] / 1000\n",
        "            win_rates.append(win_rate)\n",
        "            print(f\"Episode {episode + 1}\")\n",
        "            print(f\"Win Rate: {win_rate:.2%}\")\n",
        "            print(f\"Average Reward: {np.mean(episode_rewards[-1000:]):.2f}\")\n",
        "            print(f\"Q-table size: {len(agent.q_table)}\")\n",
        "            agent.training_stats = {'wins': 0, 'losses': 0, 'draws': 0}\n",
        "\n",
        "    # Save the trained policy if requested\n",
        "    if save_path is not None:\n",
        "        agent.save_policy(save_path)\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "id": "K5Tn2Goenw0d"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(agent: TicTacToeQLearningAgent, human_player: int = -1):\n",
        "    \"\"\"\n",
        "    Play a game against the trained agent.\n",
        "\n",
        "    Args:\n",
        "        agent: Trained Q-learning agent\n",
        "        human_player: 1 for X, -1 for O\n",
        "    \"\"\"\n",
        "    env = TicTacToeEnv()\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    print(\"Game started! You are\", \"X\" if human_player == 1 else \"O\")\n",
        "    env.render()\n",
        "\n",
        "    while not done:\n",
        "        current_player = env.current_player\n",
        "\n",
        "        if current_player == human_player:\n",
        "            valid_moves = env.get_valid_moves()\n",
        "            print(\"\\nValid moves:\", valid_moves)\n",
        "            while True:\n",
        "                try:\n",
        "                    row = int(input(\"Enter row (0-2): \"))\n",
        "                    col = int(input(\"Enter column (0-2): \"))\n",
        "                    if (row, col) in valid_moves:\n",
        "                        break\n",
        "                    print(\"Invalid move, try again.\")\n",
        "                except ValueError:\n",
        "                    print(\"Invalid input, try again.\")\n",
        "            action = (row, col)\n",
        "        else:\n",
        "            print(\"\\nAgent's turn...\")\n",
        "            valid_moves = env.get_valid_moves()\n",
        "            action = agent.get_action(state, valid_moves)\n",
        "\n",
        "        state, reward, done = env.make_move(action)\n",
        "        env.render()\n",
        "\n",
        "        if done:\n",
        "            if reward == 1:\n",
        "                winner = \"X\" if current_player == 1 else \"O\"\n",
        "                print(f\"\\nGame Over! {winner} wins!\")\n",
        "            elif reward == -1:\n",
        "                winner = \"O\" if current_player == 1 else \"X\"\n",
        "                print(f\"\\nGame Over! {winner} wins!\")\n",
        "            else:\n",
        "                print(\"\\nGame Over! It's a draw!\")"
      ],
      "metadata": {
        "id": "nGURw5Kanuvr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_environment():\n",
        "    \"\"\"\n",
        "    Test the TicTacToe environment functionality.\n",
        "\n",
        "    Executes a predefined sequence of moves and displays:\n",
        "    - Board state after each move\n",
        "    - Move position\n",
        "    - Reward received\n",
        "    - Game status\n",
        "    \"\"\"\n",
        "    env = TicTacToeEnv()\n",
        "    print(\"Initial state:\")\n",
        "    env.render()\n",
        "\n",
        "    # Test moves\n",
        "    moves = [(0, 0), (1, 1), (0, 1), (0, 2), (2, 2)]\n",
        "\n",
        "    for move in moves:\n",
        "        print(f\"\\nMove at position {move}:\")\n",
        "        board, reward, done = env.make_move(move)\n",
        "        env.render()\n",
        "        print(f\"Reward: {reward}, Game ended: {done}\")\n",
        "\n",
        "        if done:\n",
        "            break"
      ],
      "metadata": {
        "id": "xQ5z8enEnskN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Train the agent and save the policy\n",
        "    save_path = \"policies/best_policy.pkl\"\n",
        "    trained_agent = train_agent(episodes=500, save_path=save_path)\n",
        "\n",
        "    # Example of loading a saved policy\n",
        "    loaded_agent = TicTacToeQLearningAgent.load_policy(save_path)\n",
        "\n",
        "    # Play against the loaded agent\n",
        "    play_game(loaded_agent, human_player=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzsVz_3enrYR",
        "outputId": "d0b7485f-2534-401b-8163-8bc14a1f3e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy saved to: policies/best_policy.pkl\n",
            "Policy loaded from: policies/best_policy.pkl\n",
            "Game started! You are O\n",
            "---------\n",
            "|   |   |   | \n",
            "---------\n",
            "|   |   |   | \n",
            "---------\n",
            "|   |   |   | \n",
            "---------\n",
            "\n",
            "Agent's turn...\n",
            "---------\n",
            "|   |   |   | \n",
            "---------\n",
            "|   |   |   | \n",
            "---------\n",
            "|   |   | X | \n",
            "---------\n",
            "\n",
            "Valid moves: [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1)]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}